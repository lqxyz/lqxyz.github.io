\documentclass[a4paper, 1pt]{article}

%%%%%% 导入包 %%%%%%
\usepackage{graphicx}
\usepackage{amsmath} 
\usepackage{xeCJK}
\usepackage{booktabs}
\usepackage{multicol}
\setlength{\columnsep}{2em}
\usepackage{setspace} % set row space
\usepackage{geometry} % 页边距
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
\usepackage{enumitem}% item等间距过大解决办法
%\usepackage[unicode]{hyperref}
\usepackage{xcolor}
\usepackage{cite}
\usepackage{indentfirst}
\setCJKmainfont{Songti SC}
\usepackage{geometry}
\usepackage{listings}
\usepackage[T1]{fontenc}
\usepackage{courier}
\lstset{language=Python,
  backgroundcolor=\color[RGB]{245,245,244},   %代码背景色  
  basicstyle=\ttfamily\footnotesize
  =false, % 应该有bug吧，这样就不在边框上面显示字了
  breaklines=true,
  commentstyle=\color{red!50!green!50!blue!50},%浅灰色的注释
  frame=shadowbox, %把代码用带有阴影的框圈起来 
  extendedchars=false,
  lineskip=-2pt,
  keepspaces=true,
  rulesepcolor=\color{red!20!green!20!blue!20},%代码块边框为淡青色
  %backgroundcolor=\color[RGB]{198, 226, 255},   %代码背景色  
  %numbers=left,%左侧显示行号 往左靠,还可以为right，或none，即不加行号  
  %numberstyle={\color[RGB]{0,192,192}\tiny} ,%设置行号的大小，大小有tiny,scriptsize,footnotesize,small,normalsize,large等  
  keywordstyle=\color{red},
  commentstyle=\color{blue},
  morecomment=[l]{!\ }% Comment only with space after !
  %escapeinside={/*@}{@*/},  
  %escapeinside={\%*}{*)}, 
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  % stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
  % stringstyle=\color{mymauve},     % string literal style
  % tabsize=2,                       % sets default tabsize to 2 spaces
  %title=\lstname
  xleftmargin=0em, xrightmargin=0em, aboveskip=1em%设置边距
}

%%%%%% 设置字号 %%%%%%
\newcommand{\chuhao}{\fontsize{42pt}{\baselineskip}\selectfont}
\newcommand{\xiaochuhao}{\fontsize{36pt}{\baselineskip}\selectfont}
\newcommand{\yihao}{\fontsize{28pt}{\baselineskip}\selectfont}
\newcommand{\erhao}{\fontsize{21pt}{\baselineskip}\selectfont}
\newcommand{\xiaoerhao}{\fontsize{18pt}{\baselineskip}\selectfont}
\newcommand{\xiaosihao}{\fontsize{12pt}{\baselineskip}\selectfont}
\newcommand{\wuhao}{\fontsize{10.5pt}{\baselineskip}\selectfont}
\newcommand{\xiaowuhao}{\fontsize{9pt}{\baselineskip}\selectfont}
\newcommand{\liuhao}{\fontsize{7.5pt}{\baselineskip}\selectfont}

%%%% 段落首行缩进两个字 %%%%
\makeatletter
\let\@afterindentfalse\@afterindenttrue
\@afterindenttrue
\makeatother
\setlength{\parindent}{2em}  %中文缩进两个汉字位

\renewcommand{\tablename}{表}

%%%% 正文开始 %%%%
\begin{document}
\begin{spacing}{1.2}

%%%% 定义标题格式，包括title，author，affiliation，email等 %%%%
\title{\xiaosihao MNIST手写体识别实验报告\\ -- 基于Naive Bayes方法部分}
%\author{\wuhao 马佳良\ 刘群\ 胡晨琪%\footnote{}
\author{\wuhao 刘群%\footnote{}
\\[2ex]
\wuhao 地球系统科学研究中心\\
}
\date{\wuhao 2015年5月11日}
%%%% 以下部分是正文 %%%%  
\maketitle

%\tableofcontents
%\newpage
%\xiaowuhao MNIST 是一个手写体识别的数据库，里面存储了大量的关于手写体的数据，训练数据有60000张图片，测试数据有10000张图片。这里的主要任务是用Navie Bayes的方法进行手写数字的识别。
%\begin{multicols}{2}
\section{\xiaosihao Naive Bayes算法简介}
Naive Bayes算法是一种基于Bayes概率的一种分类方法，其主要思想是要最大化后验概率。主要假设所有的特征都是相互独立的，所有的特征对结论都是同等重要的。
贝叶斯分类的基本公式如下：
$$p(C|X) = \frac{p(X|C)p(C)}{p(X)} \rightarrow p(C|X) \propto p(X|C)p(C)$$
其中，$X=X(X_1, X_2, \cdots, X_n), C=C(c_1, c_2, \cdots, c_L)$. %如果$X_1, X_2, \cdots, X_n$ 是相互独立的，则有
%$$p(X|C)=p(X_1,X_2,\cdots,X_n | C) = p(X_1|C)p(X_2|C)\cdots p(X_n|C)$$
最终我们要在计算得出的后验概率$p(c_k|X)(k=1,2,\cdots,L)$中选择一个最大值，这样$X$就属于这个概率所对应的类别。
\section{\xiaosihao 几种贝叶斯实现方法的比较}
\subsection{基于sklearn库的GaussianNB算法}
这种方法是假设所有的feature都满足正态分布, 对每一种feature分别进行拟合，利用库实现的时候也比较简单，只需对模型进行训练和预测即可，代码如下(实际运行时请看代码文件 \textcolor{blue}{ nb\_sklearn\_GaussianNB.py})：
\begin{lstlisting}

#!/usr/bin/env python
import numpy as np
from sklearn.naive_bayes import GaussianNB

# 导入训练数据和标签, 前面16bits纪录的是数据的相关信息，故忽略掉
train_image = np.fromfile('train-images.idx3-ubyte', dtype=np.uint8)
train_image = (train_image[16:]).reshape([60000,784])

train_label = np.fromfile('train-labels.idx1-ubyte', dtype=np.uint8)
train_label = train_label[8:]

# 导入测试数据和标签
test_image = np.fromfile('t10k-images.idx3-ubyte', dtype=np.uint8)
test_image = (test_image[16:]).reshape([10000,784])

test_label = np.fromfile('t10k-labels.idx1-ubyte', dtype=np.uint8)
test_label = test_label[8:]

# create a Navie Bayes model based on Gaussian
model = GaussianNB()

# train the model
model.fit(train_image, train_label)

# predict 
newlabel = model.predict(test_image)
error = sum(test_label != newlabel)*1.0/10000
print "Error rate is " + str(error*100) + "%."
\end{lstlisting}

对这种方法的测试结果如下,从中可以看出，该方法的错误率高达48.76\%, 但是运行时间较短。
\begin{lstlisting}

Error rate is 48.76%.
CPU times: user 942 ms, sys: 448 ms, total: 1.39 s
Wall time: 1.44 s
\end{lstlisting}
\subsection{手动实现基于Gauss分布的NB算法(提取某些非零feature)}
在这里主要是针对上一种方法的错误率较高，想采取一种新的方法，由于在$28\times 28$的像素中，存在着一些全是0的像素点，比如说边界附近的值，这些点在实际计算中是没有什么作用的，因此可以通过一种方法去除这些点的信息，降低feature的维度。我们在这里引入了一个flag变量，其主要用来纪录不同的label所对应的训练数据中，哪些维度全是0，哪些存在着非0的feature。如果某个feature在所有该label的测试数据中全为0的话，则flag记为false，否则记为true。这样的话，在下面求Gauss分布的均值和方差时，不同的label的数据所对应的维度也不同。还有一点需要指出的是，在实际的计算中，在计算概率的对数值时，有可能出现$log 0$的情况，所以在计算的时候加入了一个小项$10^{-100}$, 这样就可以顺利的计算下去。下面是部分代码示例，实际代码在文件 \textcolor{blue}{ nb\_less\_features.py} 中。
\begin{lstlisting}

# 前面导入数据部分同上一个程序，忽略掉
# flag[i] 存放的是label为i的训练数据中哪些维度全是0
def flagCalculation(train_image, train_label):
    '''
    flag is a dictionary, where the key is the label 0-9, and the values
    are the bool values indicate which column are all zeros
    '''
    flag = {}
    for i in range(10):
        flag[i] = np.array([ train_image[train_label == i, j].sum() != 0 for j in range(784) ])
    return flag

flag = flagCalculation(train_image, train_label)

# Calculate the A prioir probablity
p_y = np.array([train_label[train_label == i].shape[0]*1.0 / train_label.shape[0] for i in range(10)])

# Calculate the sigma and mu
sigma = {}
mu = {}
# 在计算时只对feature不全是0的进行计算
for i in range(10):
    mu[i]    = np.average(train_image[:, flag[i]][train_label == i, :], axis=0)
    sigma[i] = np.std(train_image[:, flag[i]][train_label == i, :], axis=0)

def calcP_xy(x_test):
    log_p_yx = {}
    for i in range(10):
        x_test_new = x_test[flag[i]]
        # 在计算时防止出现log0，加上一个很小的数1e-100
        log_p_yx[i] = np.sum(np.log(1.0/sigma[i]/np.sqrt(2*np.pi)*np.exp(-(x_test_new-mu[i])**2/2.0/sigma[i]**2)+1e-100)) + np.log(p_y[i])
    return np.argmax(log_p_yx.values())

newLabel = np.zeros(10000)
for i in range(10000):
    newLabel[i] = calcP_xy(test_image[i,:])

error = sum(test_label != newLabel)*1.0/10000
print "Error rate is " + str(error*100) + "%."
\end{lstlisting}

对这个程序的测试表明，错误率为35.03\%, 比前一种方法提高了十三个百分点, 但是这个方法与KNN方法相比，错误率还是太高，因此下面考虑进一步改进。
\begin{lstlisting}

Error rate is 35.03%.
CPU times: user 10.7 s, sys: 228 ms, total: 11 s
Wall time: 11 s
\end{lstlisting}

\subsection{手动实现基于Gauss分布的NB算法(提取某些非零feature并归一化)}
这种实现方法与上一种算法的唯一区别是，我们对训练和测试数据进行了归一化处理，即将数据除以它们的最大值255. 下面是部分代码示例，实际代码在文件 \textcolor{blue}{ nb\_less\_features\_normalized.py} 中。
\begin{lstlisting}

train_image = np.fromfile('train-images.idx3-ubyte', dtype=np.uint8)
train_image = (train_image[16:]).reshape([60000,784]) / 255.0

test_image = np.fromfile('t10k-images.idx3-ubyte', dtype=np.uint8)
test_image = (test_image[16:]).reshape([10000,784]) / 255.0
\end{lstlisting}

我们可以看到，这样做的结果是提高了测试的准确率, 错误率由前一种方法的35.03\%, 降到了现在的21.04\%.
\begin{lstlisting}

Error rate is 21.04%.
CPU times: user 19.3 s, sys: 2.76 s, total: 22.1 s
Wall time: 22.1 s
\end{lstlisting}

\subsection{基于sklearn库中的Multinomial分布}
代码方面与基于sklearn库中的GaussianNB方法的唯一区别就是将model语句由
\begin{lstlisting}
model = GaussianNB()
\end{lstlisting}
改为了
\begin{lstlisting}
model = MultinomialNB(alpha=0.001)
\end{lstlisting}
(实际代码在文件 \textcolor{blue}{ nb\_sklearn\_MultinomialNB.py} 中。)

Multinomial Naive Bayes 分类器是应用于离散特征问题的一种算法，比如说文本单词的分类。在这个算法中，需要调节的是一个参数是$\alpha$, 这个参数是一个平滑参数，用来防止出现概率为0的情况，当$\alpha=0$时，表示没有平滑，默认值是取1，在这里经过测试之后，我取了$\alpha=0.001$, 此时，测试结果的错误率为16.3\%。同时也可以看出，这种方法的运算速度特别快，运行时间还是在ms级别。
\begin{lstlisting}

Error rate is 16.3%.
CPU times: user 386 ms, sys: 174 ms, total: 560 ms
Wall time: 384 ms
\end{lstlisting}


\subsection{基于多元正态分的NB算法}
% ----------- New paragraph -------------
由于对于这些feature的真实分布我们无法获得，因此也就很难得到真实的似然概率，在这里我们假设这些手写体数据满足多元正态分布，即
$$p(X|C)=f_{\mathbf x}(x_1,\ldots,x_n) =
\frac{1}{\sqrt{(2\pi)^n|\boldsymbol\Sigma|}}
\exp\left(-\frac{1}{2}({\mathbf x}-{\boldsymbol\mu})^\mathrm{T}{\boldsymbol\Sigma}^{-1}({\mathbf x}-{\boldsymbol\mu})\right)$$%, \quad k=1, 2,\cdots, L.
其中, $\boldsymbol\mu$ 是多维变量的均值，$\mu_i=E(X_i)$, $\boldsymbol\Sigma$ 是协方差矩阵，$|\boldsymbol\Sigma|$ 是协方差矩阵的行列式。
$$\Sigma=\mathrm{E}\left[\left(\textbf{X} - \mathrm{E}[\textbf{X}]\right)\left(\textbf{X} - \mathrm{E}[\textbf{X}]\right)^T\right] $$
$$=
\begin{bmatrix}
 \mathrm{E}[(X_1 - \mu_1)(X_1 - \mu_1)] & \mathrm{E}[(X_1 - \mu_1)(X_2 - \mu_2)] & \cdots & \mathrm{E}[(X_1 - \mu_1)(X_n - \mu_n)] \\ \\
 \mathrm{E}[(X_2 - \mu_2)(X_1 - \mu_1)] & \mathrm{E}[(X_2 - \mu_2)(X_2 - \mu_2)] & \cdots & \mathrm{E}[(X_2 - \mu_2)(X_n - \mu_n)] \\ \\
 \vdots & \vdots & \ddots & \vdots \\ \\
 \mathrm{E}[(X_n - \mu_n)(X_1 - \mu_1)] & \mathrm{E}[(X_n - \mu_n)(X_2 - \mu_2)] & \cdots & \mathrm{E}[(X_n - \mu_n)(X_n - \mu_n)]
\end{bmatrix}$$

由于在计算时，很多概率相乘可能会导致很小的值出现，因此我们并不是直接利用上面的概率来计算，而是用取对数之后的结果来表示，即
$$p(y|x) \propto p(x_i|y) = \frac{1}{\sigma_y\sqrt{2\pi}}e^{-\frac{(x_i-\mu_y)^2}{2\sigma_y^2}}\quad i=1,2,3,4$$
最后就是要计算
%$$\prod_{i=1}^4p(x_i|y)p(y)$$
%由于这样计算之后，可能出现的情况是数值特别小，这样会产生很大的误差，因此在这里我们对上式取对数，即计算
$$\begin{aligned}
\text{ln }p(c_k|X)&=\text{ln }p(X|c_k)+ \text{ln }p(c_k),\quad  k=1, 2, \cdots, L.\\
&=\text{ln }\left( \frac{1}{\sqrt{(2\pi)^n|\boldsymbol\Sigma_k|}}
\exp\left(-\frac{1}{2}({\mathbf x}-{\boldsymbol\mu_k})^\mathrm{T}{\boldsymbol\Sigma_k}^{-1}({\mathbf x}-{\boldsymbol\mu_k})\right)\right) + \text{ln }p(c_k) \\
&=-\frac{n}{2}\text{ln }2\pi -\frac{1}{2} \text{ln }|\boldsymbol\Sigma_k| -\frac{1}{2}({\mathbf x}-{\boldsymbol\mu_k})^\mathrm{T}{\boldsymbol\Sigma_k}^{-1}({\mathbf x}-{\boldsymbol\mu_k}) + \text{ln }p(c_k)
\end{aligned}$$

在实际计算中，测试数据中的各个类别的分别比较均匀，上式中有的项对于最后的结果影响较小，可以不用计算，最后就只需计算
$$
\text{ln }p(c_k|X)=-\frac{1}{2}({\mathbf x}-{\boldsymbol\mu_k})^\mathrm{T}{\boldsymbol\Sigma_k}^{-1}({\mathbf x}-{\boldsymbol\mu_k}) + \text{ln }p(c_k)
$$

在代码的实现中，我们主要调用numpy中提供的各种线性代数的函数来计算矩阵相关的运算，特别要注意的是在计算协方差矩阵时，我们在原来的基础上，在对角线上加了一个很大的数$\lambda$, 这里我们测试后发现取$15000$效果最好（如表\ref{tab_lambda}所示,注意表中的运行时间与最后实际单独运行时的时间有区别，可能是由于这里是连续测试，很多数在内存中已经存在的缘故），这样做的原因是为了防止求协方差矩阵的逆时可能出现奇异的情况。
\begin{table}[htbp]
\caption{$\lambda$与错误率和运行时间的关系\label{tab_lambda}}
\centering
\begin{tabular}{ccc}
\toprule
$\lambda$ & Error rate & Run time(unit:s)\\
\midrule
1000 & $6.21\%$  & $38.86$ \\
5000 & $4.57\%$  & $37.96$ \\
10000 & $4.17\%$  & $38.47$ \\
12000 & $4.14\%$  & $38.12$ \\
15000 & $4.06\%$  & $38.09$ \\
18000 & $4.16\%$  & $38.15$ \\
\bottomrule
\end{tabular}
\end{table}

还有就是计算矩阵的行列式耗费时间特别长，含有协方差矩阵行列式那一项对最终结果的影响比较小，因此在求解时我们就直接忽略了, 这样做提高了计算的速度，具体可以看下面的示例代码，实际代码在文件 \textcolor{blue}{ nb\_multi\_Gaussian.py} 中。
\begin{lstlisting}

#!/usr/bin/env python
import numpy as np

# Load train and test data
train_image = np.fromfile('train-images.idx3-ubyte', dtype=np.uint8)
train_image = (train_image[16:]).reshape([60000,784])

train_label = np.fromfile('train-labels.idx1-ubyte', dtype=np.uint8)
train_label = (train_label[8:])

test_image = np.fromfile('t10k-images.idx3-ubyte', dtype=np.uint8)
test_image = (test_image[16:]).reshape([10000,784])

test_label = np.fromfile('t10k-labels.idx1-ubyte', dtype=np.uint8)
test_label = (test_label[8:])

# Calculate the A prioir probablity
p_y = np.array([train_label[train_label == i].shape[0]*1.0 / train_label.shape[0] for i in range(10)])
#print p_y

# Calculate the sigma and mu
sigma = {}
mu = {}
invsigma ={}
lambda = 15000
for i in range(10):
    dat = train_image[train_label == i, :]
    mu[i]    = np.average(dat, axis=0)
    # sigma is singular, so we add some large value on the diagnoal
    sigma[i] = np.cov(dat, rowvar=0) + lambda*np.eye(784, dtype=np.uint8)
    invsigma[i] = np.linalg.inv(sigma[i])

idx = np.zeros(10000, dtype=np.uint8)

for k in range(10000):
    kdat = test_image[k,:]
    gx = np.zeros(10)
    for j in range(10):
        gx[j] = -0.5*(kdat - mu[j]).dot(invsigma[j]).dot((kdat - mu[j]).T) + np.log(p_y[j])
    idx[k] = np.argmax(gx)

err = 1 - sum(idx == test_label)*1.0/10000
print "Error rate is " + str(err*100)+ "%."
\end{lstlisting}

下面是程序运行的结果，误差率为4.06\%, 通过ipython测试耗时为 1min 14s. 可以看出，这种方法比以上所有的Naive Bayes方法的准确率都高，但是计算时间也比较长。

\begin{lstlisting}

Error rate is 4.06%.
CPU times: user 1min 10s, sys: 4.31 s, total: 1min 14s
Wall time: 43 s
\end{lstlisting}
%\section{Naive Bayes方法小结}


%\end{multicols}
\end{spacing}
\end{document}
